{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assingmnet-3 (AI42001)\n",
    "\n",
    "Submitted by Ankit Saini\n",
    "\n",
    "17BT30002 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Sentence classification: \n",
    "\n",
    "Consider the files traindata.csv and testdata.csv. In these files, each row contains a sentence which belongs to one of 4 categories (science, sports, business, covid crisis). Learn a Naive Bayes classifier to predict the category of each sentence, based on the words in it (neglecting stop words). Use the training set to estimate the prior distribution over the class labels and class-conditional probabilities, i.e. the probability of each word occurring in a sentence having a particular class label. For each test sentence, your output should be the posterior distribution over the labels.\n",
    "\n",
    "_[Trick: never set p(w|Y=k)=0 for any word w and label k, even if word w never exists in any sentence with label k. Assign a small probability like 0.01. Adjust the probabilities of other words too, such that you get a proper conditional distribution]_\n",
    "\n",
    "1. Construct the vocabulary without stop-words [2 marks]\n",
    "\n",
    "2. Calculate the prior distribution of the labels [1 mark]\n",
    "\n",
    "3. Calculate the class-conditional probabilities of each word in the vocabulary, for each topic [4 marks]\n",
    "\n",
    "4. For each test sentence, create the posterior distribution over the labels [3 marks]\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv('traindata.csv') #reading the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>Outer space is not friendly to life. Extreme t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Tennis, original name lawn tennis, game in whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>One woman who frequently flew on Southwest was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid</td>\n",
       "      <td>In December 2019, almost seven years after the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Any life-forms that somehow find themselves in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0   science  Outer space is not friendly to life. Extreme t...\n",
       "1    sports  Tennis, original name lawn tennis, game in whi...\n",
       "2  business  One woman who frequently flew on Southwest was...\n",
       "3     covid  In December 2019, almost seven years after the...\n",
       "4   science  Any life-forms that somehow find themselves in..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Require only two columns with respect to others \n",
    "train_data_df = train_data_df.iloc[:, 0:2] \n",
    "train_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = train_data_df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science 20\n",
      "sports 20\n",
      "covid 21\n",
      "business 19\n"
     ]
    }
   ],
   "source": [
    "label_1 = \"science\"\n",
    "label_2 = \"sports\"\n",
    "label_3 = \"covid\"\n",
    "label_4 = \"business\"\n",
    "count_1 = count_2 = count_3 = count_4 = 0\n",
    "for i in range(80):\n",
    "    if category[i] == label_1:\n",
    "        count_1 = count_1 + 1\n",
    "    elif category[i] == label_2:\n",
    "        count_2 = count_2 + 1\n",
    "    elif category[i] == label_3:\n",
    "        count_3 = count_3 + 1\n",
    "    elif category[i] == label_4:\n",
    "        count_4 = count_4 + 1\n",
    "        \n",
    "print ( label_1, count_1)\n",
    "print ( label_2, count_2)\n",
    "print ( label_3, count_3)\n",
    "print ( label_4, count_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'science': 20, 'sports': 20, 'business': 19, 'covid': 21})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comaprsion with the manually calculated count values of labels\n",
    "category_dic = Counter(category)\n",
    "category_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct a Vocab without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In December 2019, almost seven years after the MERS 2012 outbreak, a novel Coronavirus (2019-nCoV) surfaced in Wuhan in the Hubei region of China.\n",
      "---------------\n",
      "Modified text after removing un-necessary words\n",
      "---------------\n",
      "In December       almost seven years after the MERS      outbreak  a novel Coronavirus       nCoV  surfaced in Wuhan in the Hubei region of China \n"
     ]
    }
   ],
   "source": [
    "#Importing the module of perl-like regular expression\n",
    "import re\n",
    "print (train_data_df.text[3])\n",
    "letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", train_data_df.text[3])\n",
    "print (\"---------------\")\n",
    "print (\"Modified text after removing un-necessary words\")\n",
    "print (\"---------------\")\n",
    "print (letters_only_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Stopwords list using nltk module\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_txt):\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_txt)\n",
    "    words = letters_only_text.lower().split()\n",
    "    modified_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            modified_words.append(word)\n",
    "            \n",
    "    return \" \".join(modified_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['new_text'] = train_data_df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>Outer space is not friendly to life. Extreme t...</td>\n",
       "      <td>outer space friendly life extreme temperatures...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Tennis, original name lawn tennis, game in whi...</td>\n",
       "      <td>tennis original name lawn tennis game two oppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>One woman who frequently flew on Southwest was...</td>\n",
       "      <td>one woman frequently flew southwest constantly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid</td>\n",
       "      <td>In December 2019, almost seven years after the...</td>\n",
       "      <td>december almost seven years mers outbreak nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Any life-forms that somehow find themselves in...</td>\n",
       "      <td>life forms somehow find void soon die unless b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0   science  Outer space is not friendly to life. Extreme t...   \n",
       "1    sports  Tennis, original name lawn tennis, game in whi...   \n",
       "2  business  One woman who frequently flew on Southwest was...   \n",
       "3     covid  In December 2019, almost seven years after the...   \n",
       "4   science  Any life-forms that somehow find themselves in...   \n",
       "\n",
       "                                            new_text  \n",
       "0  outer space friendly life extreme temperatures...  \n",
       "1  tennis original name lawn tennis game two oppo...  \n",
       "2  one woman frequently flew southwest constantly...  \n",
       "3  december almost seven years mers outbreak nove...  \n",
       "4  life forms somehow find void soon die unless b...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lst = []\n",
    "for i in train_data_df.new_text:\n",
    "    words_lst = words_lst + i.split()\n",
    "    \n",
    "Vocab_lst = words_lst #Vocabulary List without any special characters & stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outer',\n",
       " 'space',\n",
       " 'friendly',\n",
       " 'life',\n",
       " 'extreme',\n",
       " 'temperatures',\n",
       " 'low',\n",
       " 'pressure',\n",
       " 'radiation',\n",
       " 'quickly']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab_lst[:10] #Vocabulary list with out stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'outer': 5,\n",
       "         'space': 10,\n",
       "         'friendly': 1,\n",
       "         'life': 4,\n",
       "         'extreme': 2,\n",
       "         'temperatures': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'radiation': 4,\n",
       "         'quickly': 1,\n",
       "         'degrade': 1,\n",
       "         'cell': 1,\n",
       "         'membranes': 1,\n",
       "         'destroy': 1,\n",
       "         'dna': 2,\n",
       "         'tennis': 17,\n",
       "         'original': 1,\n",
       "         'name': 1,\n",
       "         'lawn': 3,\n",
       "         'game': 9,\n",
       "         'two': 3,\n",
       "         'opposing': 1,\n",
       "         'players': 6,\n",
       "         'singles': 2,\n",
       "         'pairs': 1,\n",
       "         'doubles': 2,\n",
       "         'use': 5,\n",
       "         'tautly': 1,\n",
       "         'strung': 1,\n",
       "         'rackets': 1,\n",
       "         'hit': 1,\n",
       "         'ball': 8,\n",
       "         'specified': 1,\n",
       "         'size': 1,\n",
       "         'weight': 3,\n",
       "         'bounce': 1,\n",
       "         'net': 6,\n",
       "         'rectangular': 1,\n",
       "         'court': 13,\n",
       "         'one': 5,\n",
       "         'woman': 1,\n",
       "         'frequently': 1,\n",
       "         'flew': 1,\n",
       "         'southwest': 4,\n",
       "         'constantly': 1,\n",
       "         'disappointed': 1,\n",
       "         'every': 7,\n",
       "         'aspect': 1,\n",
       "         'company': 4,\n",
       "         'operation': 1,\n",
       "         'fact': 2,\n",
       "         'became': 1,\n",
       "         'known': 5,\n",
       "         'pen': 1,\n",
       "         'pal': 1,\n",
       "         'flight': 1,\n",
       "         'wrote': 1,\n",
       "         'complaint': 1,\n",
       "         'december': 2,\n",
       "         'almost': 2,\n",
       "         'seven': 1,\n",
       "         'years': 4,\n",
       "         'mers': 5,\n",
       "         'outbreak': 4,\n",
       "         'novel': 1,\n",
       "         'coronavirus': 5,\n",
       "         'ncov': 2,\n",
       "         'surfaced': 1,\n",
       "         'wuhan': 3,\n",
       "         'hubei': 2,\n",
       "         'region': 2,\n",
       "         'china': 5,\n",
       "         'forms': 2,\n",
       "         'somehow': 2,\n",
       "         'find': 1,\n",
       "         'void': 1,\n",
       "         'soon': 1,\n",
       "         'die': 1,\n",
       "         'unless': 1,\n",
       "         'band': 1,\n",
       "         'together': 1,\n",
       "         'small': 2,\n",
       "         'communities': 1,\n",
       "         'new': 4,\n",
       "         'research': 2,\n",
       "         'shows': 1,\n",
       "         'bacteria': 5,\n",
       "         'withstand': 1,\n",
       "         'harsh': 1,\n",
       "         'environment': 1,\n",
       "         'balls': 4,\n",
       "         'deinococcus': 2,\n",
       "         'thin': 1,\n",
       "         'five': 1,\n",
       "         'sheets': 1,\n",
       "         'paper': 1,\n",
       "         'placed': 2,\n",
       "         'outside': 3,\n",
       "         'international': 3,\n",
       "         'station': 2,\n",
       "         'stayed': 1,\n",
       "         'three': 2,\n",
       "         'microbes': 6,\n",
       "         'heart': 1,\n",
       "         'survived': 2,\n",
       "         'group': 3,\n",
       "         'layers': 2,\n",
       "         'shielded': 2,\n",
       "         'extremes': 1,\n",
       "         'could': 2,\n",
       "         'survive': 3,\n",
       "         'inside': 1,\n",
       "         'artificial': 1,\n",
       "         'meteorites': 1,\n",
       "         'first': 8,\n",
       "         'evidence': 1,\n",
       "         'long': 3,\n",
       "         'unprotected': 1,\n",
       "         'says': 5,\n",
       "         'margaret': 1,\n",
       "         'cramm': 2,\n",
       "         'suggests': 1,\n",
       "         'microbiologist': 1,\n",
       "         'university': 1,\n",
       "         'calgary': 1,\n",
       "         'canada': 1,\n",
       "         'take': 1,\n",
       "         'part': 2,\n",
       "         'study': 1,\n",
       "         'finding': 1,\n",
       "         'adds': 1,\n",
       "         'worry': 1,\n",
       "         'human': 1,\n",
       "         'travel': 1,\n",
       "         'accidentally': 1,\n",
       "         'introduce': 1,\n",
       "         'planets': 1,\n",
       "         'akihiko': 1,\n",
       "         'yamagishi': 2,\n",
       "         'astrobiologist': 1,\n",
       "         'works': 1,\n",
       "         'institute': 1,\n",
       "         'astronautical': 1,\n",
       "         'science': 1,\n",
       "         'tokyo': 1,\n",
       "         'japan': 1,\n",
       "         'team': 2,\n",
       "         'sent': 2,\n",
       "         'dried': 1,\n",
       "         'pellets': 5,\n",
       "         'bacteriato': 1,\n",
       "         'resistant': 1,\n",
       "         'thrive': 1,\n",
       "         'places': 2,\n",
       "         'earth': 2,\n",
       "         'stratosphere': 1,\n",
       "         'stuffed': 1,\n",
       "         'wells': 1,\n",
       "         'metal': 1,\n",
       "         'plates': 2,\n",
       "         'nasa': 1,\n",
       "         'astronaut': 1,\n",
       "         'scott': 1,\n",
       "         'kelly': 1,\n",
       "         'affixed': 1,\n",
       "         'exterior': 1,\n",
       "         'samples': 1,\n",
       "         'back': 5,\n",
       "         'year': 2,\n",
       "         'home': 1,\n",
       "         'researchers': 2,\n",
       "         'moistened': 1,\n",
       "         'also': 5,\n",
       "         'fed': 1,\n",
       "         'food': 1,\n",
       "         'waited': 1,\n",
       "         'micrometer': 1,\n",
       "         'thick': 2,\n",
       "         'make': 4,\n",
       "         'studies': 1,\n",
       "         'suggested': 1,\n",
       "         'fried': 1,\n",
       "         'genetic': 1,\n",
       "         'material': 1,\n",
       "         'micrometers': 1,\n",
       "         'inch': 1,\n",
       "         'dead': 2,\n",
       "         'discolored': 1,\n",
       "         'ultraviolet': 1,\n",
       "         'desiccation': 1,\n",
       "         'cells': 2,\n",
       "         'inner': 1,\n",
       "         'hazards': 1,\n",
       "         'four': 1,\n",
       "         'larger': 1,\n",
       "         'points': 1,\n",
       "         'awarded': 1,\n",
       "         'player': 1,\n",
       "         'whenever': 1,\n",
       "         'opponent': 6,\n",
       "         'fails': 1,\n",
       "         'correctly': 1,\n",
       "         'return': 2,\n",
       "         'within': 2,\n",
       "         'prescribed': 1,\n",
       "         'dimensions': 2,\n",
       "         'organized': 1,\n",
       "         'played': 4,\n",
       "         'according': 1,\n",
       "         'rules': 1,\n",
       "         'sanctioned': 1,\n",
       "         'federation': 2,\n",
       "         'itf': 3,\n",
       "         'world': 1,\n",
       "         'governing': 1,\n",
       "         'body': 1,\n",
       "         'sport': 3,\n",
       "         'originally': 3,\n",
       "         'formally': 1,\n",
       "         'still': 3,\n",
       "         'britain': 2,\n",
       "         'grass': 2,\n",
       "         'courts': 3,\n",
       "         'victorian': 1,\n",
       "         'gentlemen': 1,\n",
       "         'ladies': 2,\n",
       "         'variety': 2,\n",
       "         'surfaces': 2,\n",
       "         'origins': 2,\n",
       "         'traced': 1,\n",
       "         'th': 2,\n",
       "         'century': 2,\n",
       "         'french': 1,\n",
       "         'handball': 1,\n",
       "         'called': 4,\n",
       "         'jeu': 1,\n",
       "         'de': 1,\n",
       "         'paume': 1,\n",
       "         'palm': 1,\n",
       "         'derived': 1,\n",
       "         'complex': 1,\n",
       "         'indoor': 1,\n",
       "         'racket': 2,\n",
       "         'real': 2,\n",
       "         'ancient': 1,\n",
       "         'limited': 3,\n",
       "         'degree': 1,\n",
       "         'usually': 2,\n",
       "         'united': 2,\n",
       "         'states': 2,\n",
       "         'royal': 1,\n",
       "         'australia': 1,\n",
       "         'period': 1,\n",
       "         'rapid': 1,\n",
       "         'growth': 1,\n",
       "         'participant': 1,\n",
       "         'spectator': 1,\n",
       "         'began': 1,\n",
       "         'late': 1,\n",
       "         'major': 1,\n",
       "         'championships': 1,\n",
       "         'opened': 1,\n",
       "         'professionals': 1,\n",
       "         'well': 3,\n",
       "         'amateurs': 1,\n",
       "         'continued': 1,\n",
       "         'television': 1,\n",
       "         'broadcasts': 1,\n",
       "         'expanding': 1,\n",
       "         'professional': 1,\n",
       "         'tournament': 2,\n",
       "         'circuits': 1,\n",
       "         'rise': 1,\n",
       "         'notable': 1,\n",
       "         'rivalries': 1,\n",
       "         'broadened': 1,\n",
       "         'appeal': 1,\n",
       "         'enjoyed': 1,\n",
       "         'practically': 1,\n",
       "         'level': 1,\n",
       "         'skill': 1,\n",
       "         'top': 2,\n",
       "         'competition': 1,\n",
       "         'demanding': 2,\n",
       "         'test': 1,\n",
       "         'shot': 1,\n",
       "         'making': 1,\n",
       "         'stamina': 1,\n",
       "         'rich': 1,\n",
       "         'stylistic': 1,\n",
       "         'strategic': 1,\n",
       "         'garden': 1,\n",
       "         'party': 1,\n",
       "         'whalebone': 1,\n",
       "         'corsets': 1,\n",
       "         'starched': 1,\n",
       "         'petticoats': 1,\n",
       "         'men': 1,\n",
       "         'white': 2,\n",
       "         'flannels': 1,\n",
       "         'evolved': 1,\n",
       "         'physical': 1,\n",
       "         'chess': 1,\n",
       "         'match': 1,\n",
       "         'attack': 1,\n",
       "         'defend': 1,\n",
       "         'exploiting': 1,\n",
       "         'angles': 1,\n",
       "         'technical': 1,\n",
       "         'weaknesses': 1,\n",
       "         'strokes': 1,\n",
       "         'widely': 1,\n",
       "         'diverse': 1,\n",
       "         'pace': 1,\n",
       "         'spin': 2,\n",
       "         'national': 1,\n",
       "         'associations': 1,\n",
       "         'constitute': 1,\n",
       "         'govern': 1,\n",
       "         'worldwide': 1,\n",
       "         'oversee': 1,\n",
       "         'competitions': 1,\n",
       "         'davis': 1,\n",
       "         'cup': 2,\n",
       "         'olympic': 1,\n",
       "         'games': 4,\n",
       "         'restored': 1,\n",
       "         'medal': 1,\n",
       "         'status': 1,\n",
       "         'time': 3,\n",
       "         'since': 2,\n",
       "         'feet': 6,\n",
       "         'metres': 2,\n",
       "         'height': 1,\n",
       "         'centre': 1,\n",
       "         'metre': 2,\n",
       "         'supported': 1,\n",
       "         'side': 5,\n",
       "         'posts': 1,\n",
       "         'high': 2,\n",
       "         'common': 3,\n",
       "         'materials': 1,\n",
       "         'today': 1,\n",
       "         'clay': 1,\n",
       "         'hard': 2,\n",
       "         'although': 1,\n",
       "         'term': 1,\n",
       "         'refers': 1,\n",
       "         'surface': 3,\n",
       "         'cement': 1,\n",
       "         'number': 2,\n",
       "         'cushioned': 1,\n",
       "         'asphalt': 1,\n",
       "         'derivatives': 1,\n",
       "         'synthetic': 1,\n",
       "         'consists': 1,\n",
       "         'pressurized': 1,\n",
       "         'rubber': 1,\n",
       "         'core': 1,\n",
       "         'covered': 1,\n",
       "         'quality': 1,\n",
       "         'cloth': 1,\n",
       "         'wool': 1,\n",
       "         'mixed': 2,\n",
       "         'percent': 1,\n",
       "         'nylon': 1,\n",
       "         'gradually': 1,\n",
       "         'go': 1,\n",
       "         'soft': 1,\n",
       "         'play': 1,\n",
       "         'changed': 1,\n",
       "         'regular': 1,\n",
       "         'intervals': 1,\n",
       "         'agreed': 1,\n",
       "         'upon': 2,\n",
       "         'officials': 1,\n",
       "         'depending': 1,\n",
       "         'factors': 1,\n",
       "         'must': 3,\n",
       "         'uniform': 1,\n",
       "         'seams': 1,\n",
       "         'stitchless': 1,\n",
       "         'specifies': 1,\n",
       "         'yellow': 1,\n",
       "         'inches': 1,\n",
       "         'cm': 1,\n",
       "         'diameter': 1,\n",
       "         'ounces': 1,\n",
       "         'grams': 1,\n",
       "         'opponents': 1,\n",
       "         'toss': 1,\n",
       "         'coin': 1,\n",
       "         'decide': 3,\n",
       "         'service': 15,\n",
       "         'winner': 1,\n",
       "         'may': 4,\n",
       "         'serve': 3,\n",
       "         'receive': 2,\n",
       "         'case': 2,\n",
       "         'chooses': 1,\n",
       "         'choice': 2,\n",
       "         'choose': 1,\n",
       "         'alternate': 1,\n",
       "         'change': 1,\n",
       "         'sides': 1,\n",
       "         'odd': 1,\n",
       "         'beginning': 1,\n",
       "         'behind': 2,\n",
       "         'right': 9,\n",
       "         'hand': 2,\n",
       "         'server': 2,\n",
       "         'baseline': 1,\n",
       "         'strikes': 2,\n",
       "         'diagonally': 1,\n",
       "         'across': 1,\n",
       "         'strike': 1,\n",
       "         'falling': 1,\n",
       "         'correct': 2,\n",
       "         'let': 1,\n",
       "         'replayed': 1,\n",
       "         'allowed': 1,\n",
       "         'miss': 1,\n",
       "         'fault': 1,\n",
       "         'either': 2,\n",
       "         'failure': 1,\n",
       "         'deliver': 1,\n",
       "         'attempts': 1,\n",
       "         'constitutes': 1,\n",
       "         'loss': 1,\n",
       "         'point': 1,\n",
       "         'receiver': 1,\n",
       "         'hits': 1,\n",
       "         'ground': 1,\n",
       "         'second': 1,\n",
       "         'boundaries': 1,\n",
       "         'like': 4,\n",
       "         'assign': 1,\n",
       "         'seats': 1,\n",
       "         'last': 1,\n",
       "         'letter': 1,\n",
       "         'reciting': 1,\n",
       "         'litany': 1,\n",
       "         'complaints': 1,\n",
       "         'momentarily': 1,\n",
       "         'stumped': 1,\n",
       "         'customer': 15,\n",
       "         'relations': 1,\n",
       "         'people': 6,\n",
       "         'bumped': 1,\n",
       "         'herb': 1,\n",
       "         'kelleher': 2,\n",
       "         'ceo': 1,\n",
       "         'desk': 1,\n",
       "         'note': 2,\n",
       "         'phrase': 2,\n",
       "         'always': 6,\n",
       "         'coined': 1,\n",
       "         'harry': 1,\n",
       "         'gordon': 1,\n",
       "         'selfridge': 2,\n",
       "         'founder': 1,\n",
       "         'department': 1,\n",
       "         'store': 1,\n",
       "         'london': 1,\n",
       "         'typically': 1,\n",
       "         'used': 1,\n",
       "         'businesses': 3,\n",
       "         'convince': 2,\n",
       "         'customers': 11,\n",
       "         'get': 2,\n",
       "         'good': 3,\n",
       "         'employees': 13,\n",
       "         'give': 3,\n",
       "         'however': 3,\n",
       "         'think': 3,\n",
       "         'abandon': 1,\n",
       "         'ironically': 1,\n",
       "         'leads': 1,\n",
       "         'worse': 1,\n",
       "         'wanted': 1,\n",
       "         'sure': 1,\n",
       "         'liked': 1,\n",
       "         'way': 1,\n",
       "         'continental': 2,\n",
       "         'treated': 2,\n",
       "         'made': 1,\n",
       "         'clear': 1,\n",
       "         'maxim': 2,\n",
       "         'hold': 1,\n",
       "         'sway': 1,\n",
       "         'run': 3,\n",
       "         'reel': 1,\n",
       "         'loyalty': 1,\n",
       "         'put': 4,\n",
       "         'stuff': 1,\n",
       "         'day': 2,\n",
       "         'buy': 1,\n",
       "         'ticket': 2,\n",
       "         'abuse': 1,\n",
       "         'million': 1,\n",
       "         'books': 1,\n",
       "         'month': 1,\n",
       "         'going': 2,\n",
       "         'unreasonable': 2,\n",
       "         'jerks': 1,\n",
       "         'supporting': 1,\n",
       "         'work': 3,\n",
       "         'product': 1,\n",
       "         'irate': 2,\n",
       "         'jerk': 1,\n",
       "         'demands': 1,\n",
       "         'free': 1,\n",
       "         'paris': 1,\n",
       "         'ran': 1,\n",
       "         'peanuts': 1,\n",
       "         'whose': 1,\n",
       "         'treat': 1,\n",
       "         'serfs': 1,\n",
       "         'value': 1,\n",
       "         'support': 1,\n",
       "         'line': 1,\n",
       "         'even': 3,\n",
       "         'smallest': 1,\n",
       "         'problem': 1,\n",
       "         'cause': 3,\n",
       "         'resentment': 2,\n",
       "         'bethune': 2,\n",
       "         'trusted': 1,\n",
       "         'attitude': 1,\n",
       "         'balances': 1,\n",
       "         'squarely': 1,\n",
       "         'favors': 1,\n",
       "         'bad': 4,\n",
       "         'idea': 1,\n",
       "         'causes': 1,\n",
       "         'among': 1,\n",
       "         'course': 1,\n",
       "         'plenty': 1,\n",
       "         'examples': 1,\n",
       "         'giving': 1,\n",
       "         'lousy': 1,\n",
       "         'trying': 2,\n",
       "         'solve': 1,\n",
       "         'declaring': 1,\n",
       "         'counter': 1,\n",
       "         'productive': 1,\n",
       "         'using': 1,\n",
       "         'slogan': 1,\n",
       "         'abusive': 2,\n",
       "         'demand': 1,\n",
       "         'anything': 1,\n",
       "         'definition': 1,\n",
       "         'makes': 2,\n",
       "         'jobs': 1,\n",
       "         'much': 2,\n",
       "         'harder': 1,\n",
       "         'rein': 1,\n",
       "         'means': 1,\n",
       "         'better': 3,\n",
       "         'treatment': 2,\n",
       "         'conditions': 1,\n",
       "         'nice': 3,\n",
       "         'seemed': 1,\n",
       "         'wrong': 1,\n",
       "         'sense': 2,\n",
       "         'keep': 1,\n",
       "         'coming': 1,\n",
       "         'quite': 1,\n",
       "         'simply': 1,\n",
       "         'business': 1,\n",
       "         'technicians': 1,\n",
       "         'arrived': 1,\n",
       "         'site': 1,\n",
       "         'maintenance': 1,\n",
       "         'task': 2,\n",
       "         'great': 1,\n",
       "         'shock': 1,\n",
       "         'rudely': 1,\n",
       "         'finished': 1,\n",
       "         'returned': 1,\n",
       "         'office': 1,\n",
       "         'told': 1,\n",
       "         'management': 1,\n",
       "         'experience': 2,\n",
       "         'promptly': 1,\n",
       "         'cancelled': 1,\n",
       "         'contract': 1,\n",
       "         'dismissed': 1,\n",
       "         'lady': 1,\n",
       "         'kept': 2,\n",
       "         'complaining': 1,\n",
       "         'flying': 1,\n",
       "         'servicegruppen': 1,\n",
       "         'fired': 1,\n",
       "         'matter': 1,\n",
       "         'financial': 1,\n",
       "         'calculation': 1,\n",
       "         'question': 1,\n",
       "         'whether': 1,\n",
       "         'would': 1,\n",
       "         'lose': 1,\n",
       "         'money': 1,\n",
       "         'rosenbluth': 1,\n",
       "         'argues': 1,\n",
       "         'happy': 2,\n",
       "         'cov': 4,\n",
       "         'large': 1,\n",
       "         'family': 3,\n",
       "         'viruses': 8,\n",
       "         'illnesses': 1,\n",
       "         'ranging': 1,\n",
       "         'cold': 2,\n",
       "         'acute': 3,\n",
       "         'respiratory': 6,\n",
       "         'tract': 1,\n",
       "         'infection': 2,\n",
       "         'severity': 1,\n",
       "         'visible': 1,\n",
       "         'pneumonia': 3,\n",
       "         'syndrome': 3,\n",
       "         'death': 1,\n",
       "         'sars': 5,\n",
       "         'greatly': 1,\n",
       "         'overlooked': 1,\n",
       "         'outbreaks': 1,\n",
       "         'studied': 1,\n",
       "         'greater': 2,\n",
       "         'detail': 1,\n",
       "         'propelling': 1,\n",
       "         'vaccine': 1,\n",
       "         'mysterious': 1,\n",
       "         'cases': 1,\n",
       "         'detected': 1,\n",
       "         'city': 1,\n",
       "         'province': 2,\n",
       "         'january': 1,\n",
       "         'causative': 1,\n",
       "         'agent': 1,\n",
       "         'identified': 1,\n",
       "         'disease': 1,\n",
       "         'later': 2,\n",
       "         'named': 1,\n",
       "         'covid': 1,\n",
       "         'virus': 6,\n",
       "         'spread': 1,\n",
       "         'extensively': 1,\n",
       "         'gained': 1,\n",
       "         'entry': 2,\n",
       "         'countries': 2,\n",
       "         'territories': 1,\n",
       "         'though': 1,\n",
       "         'experts': 1,\n",
       "         'suspected': 1,\n",
       "         'transmitted': 1,\n",
       "         'animals': 2,\n",
       "         'humans': 2,\n",
       "         'reports': 1,\n",
       "         'origin': 3,\n",
       "         'options': 1,\n",
       "         'available': 1,\n",
       "         'anti': 1,\n",
       "         'hiv': 1,\n",
       "         'drugs': 1,\n",
       "         'antivirals': 1,\n",
       "         'remdesivir': 1,\n",
       "         'galidesivir': 1,\n",
       "         'containment': 1,\n",
       "         'recommended': 1,\n",
       "         'quarantine': 1,\n",
       "         'infected': 1,\n",
       "         'follow': 1,\n",
       "         'hygiene': 1,\n",
       "         'practices': 1,\n",
       "         'significant': 1,\n",
       "         'socio': 1,\n",
       "         'economic': 1,\n",
       "         'impact': 1,\n",
       "         'globally': 1,\n",
       "         'economically': 1,\n",
       "         'likely': 1,\n",
       "         'setback': 1,\n",
       "         'pandemic': 1,\n",
       "         'due': 1,\n",
       "         'added': 1,\n",
       "         'trade': 1,\n",
       "         'war': 1,\n",
       "         'coronaviridae': 1,\n",
       "         'positive': 1,\n",
       "         'rna': 1,\n",
       "         'possess': 1,\n",
       "         'viral': 1,\n",
       "         'coat': 1,\n",
       "         'looked': 2,\n",
       "         'help': 1,\n",
       "         'electron': 1,\n",
       "         'microscope': 1,\n",
       "         'appears': 1,\n",
       "         'unique': 2,\n",
       "         'corona': 1,\n",
       "         'around': 1,\n",
       "         'mainly': 1,\n",
       "         'diseases': 1,\n",
       "         'infections': 1,\n",
       "         'infect': 1,\n",
       "         'attracted': 1,\n",
       "         'interest': 2,\n",
       "         'severe': 1,\n",
       "         'caused': 2,\n",
       "         'renewed': 1,\n",
       "         'happened': 1,\n",
       "         'epidemic': 1,\n",
       "         'st': 1,\n",
       "         'originating': 1,\n",
       "         'guangdong': 1,\n",
       "         'middle': 1,\n",
       "         'east': 1,\n",
       "         'zoonotic': 2,\n",
       "         'originated': 1,\n",
       "         'bats': 1,\n",
       "         'feature': 1,\n",
       "         'ability': 1,\n",
       "         'mutate': 1,\n",
       "         'rapidly': 1,\n",
       "         'adapt': 1,\n",
       "         'host': 3,\n",
       "         'allows': 1,\n",
       "         'jump': 1,\n",
       "         'coronaviruses': 1,\n",
       "         'angiotensin': 1,\n",
       "         'converting': 1,\n",
       "         'enzyme': 1,\n",
       "         'ace': 1,\n",
       "         'receptor': 1,\n",
       "         'dipeptidyl': 1,\n",
       "         'peptidase': 1,\n",
       "         'iv': 1,\n",
       "         'dpp': 1,\n",
       "         'protein': 1,\n",
       "         'gain': 1,\n",
       "         'replication': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking frequency of words in a list\n",
    "words_dic = Counter(Vocab_lst)\n",
    "words_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the prior distribution of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_y = train_data_df.shape\n",
    "\n",
    "def prior_distribution(train_data_df):\n",
    "    \n",
    "    prior_dist = {}\n",
    "    prior_list = []\n",
    "    \n",
    "    for i in train_data_df.category.unique():\n",
    "        prior_dist[i] = train_data_df[train_data_df.category == i].shape[0]/total_y[0] \n",
    "        prior_list.append(train_data_df[train_data_df.category == i].shape[0]/total_y[0])\n",
    "        \n",
    "    return prior_dist, prior_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dist, prior_list = prior_distribution(train_data_df) #prior distribution of label\n",
    "index = ['Occurance', 'Frequency']\n",
    "prior_list = np.array(prior_list)\n",
    "\n",
    "prior_dic = {}\n",
    "\n",
    "for i in category_dic.keys():\n",
    "    prior_dic[i] = [category_dic[i], prior_dist[i]]\n",
    "    \n",
    "    \n",
    "prior_df = pd.DataFrame(prior_dic, index = index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Prior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>science</th>\n",
       "      <th>sports</th>\n",
       "      <th>business</th>\n",
       "      <th>covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Occurance</th>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>19.0000</td>\n",
       "      <td>21.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>0.2625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           science  sports  business    covid\n",
       "Occurance    20.00   20.00   19.0000  21.0000\n",
       "Frequency     0.25    0.25    0.2375   0.2625"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate the class-conditional probabilities of each word in the vocabulary, for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_per_label(data):\n",
    "    \n",
    "    data_lst = []\n",
    "\n",
    "    for text in data:\n",
    "        data_lst = data_lst + text.split()\n",
    "        \n",
    "    data_dic = Counter(data_lst)\n",
    "    \n",
    "    return data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_distribution(data_dic, words_dic):\n",
    "    \n",
    "    conditional_prob = []\n",
    "    \n",
    "    for word in words_dic.keys():\n",
    "        if word in data_dic.keys():\n",
    "            conditional_prob.append(float('{:04f}'.format(data_dic[word]/words_dic[word])))\n",
    "        else:\n",
    "            #Replacing Values of words in class-conditional distribution having zero probability \n",
    "            conditional_prob.append(0.01)\n",
    "    \n",
    "    return conditional_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_dic = {}\n",
    "\n",
    "for i in train_data_df.category.unique():\n",
    "    \n",
    "    data_dic = count_words_per_label(train_data_df[train_data_df.category == i].new_text)\n",
    "    \n",
    "    conditional_dic[i] = conditional_distribution(data_dic, words_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-conditional DataFrame with Replaced Values of words in class-conditional distribution, those having zero probabilities. \n",
    "\n",
    "> ### Class-conditional Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>science</th>\n",
       "      <th>sports</th>\n",
       "      <th>business</th>\n",
       "      <th>covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>outer</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friendly</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extreme</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperatures</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pressure</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radiation</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quickly</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrade</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>membranes</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>destroy</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dna</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tennis</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lawn</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>game</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              science  sports  business  covid\n",
       "outer            0.60    0.20      0.01   0.20\n",
       "space            1.00    0.01      0.01   0.01\n",
       "friendly         1.00    0.01      0.01   0.01\n",
       "life             1.00    0.01      0.01   0.01\n",
       "extreme          1.00    0.01      0.01   0.01\n",
       "temperatures     1.00    0.01      0.01   0.01\n",
       "low              1.00    0.01      0.01   0.01\n",
       "pressure         0.50    0.01      0.01   0.50\n",
       "radiation        1.00    0.01      0.01   0.01\n",
       "quickly          1.00    0.01      0.01   0.01\n",
       "degrade          1.00    0.01      0.01   0.01\n",
       "cell             1.00    0.01      0.01   0.01\n",
       "membranes        1.00    0.01      0.01   0.01\n",
       "destroy          1.00    0.01      0.01   0.01\n",
       "dna              1.00    0.01      0.01   0.01\n",
       "tennis           0.01    1.00      0.01   0.01\n",
       "original         0.01    1.00      0.01   0.01\n",
       "name             0.01    1.00      0.01   0.01\n",
       "lawn             0.01    1.00      0.01   0.01\n",
       "game             0.01    1.00      0.01   0.01"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_df = pd.DataFrame(conditional_dic, index = words_dic.keys())\n",
    "#df.insert(0, \"Words\", words_lst)\n",
    "conditional_df.head(20) #class - conditional probabilities of each word in vocab  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. For each test sentence, create the posterior distribution over the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_distribution(data):\n",
    "    \n",
    "    posterior_val = []\n",
    "    \n",
    "    for j in range(4):\n",
    "        \n",
    "        res = 1\n",
    "        \n",
    "        for word in data.split():\n",
    "            \n",
    "            row_data = np.array(conditional_df.loc[word][0:4])\n",
    "            res = res * (prior_list[j]*row_data[j]/np.dot(row_data, prior_list))\n",
    "            \n",
    "        posterior_val.append((res))\n",
    "        \n",
    "    return posterior_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_list = []\n",
    "\n",
    "for data in train_data_df.new_text:\n",
    "    posterior_list.append(posterior_distribution(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_df = pd.DataFrame(posterior_list, columns = ['science', 'sports', 'business', 'covid'])\n",
    "\n",
    "posterior_df.insert(0, \"Modified Text\", train_data_df.new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Posterior Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modified Text</th>\n",
       "      <th>science</th>\n",
       "      <th>sports</th>\n",
       "      <th>business</th>\n",
       "      <th>covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>outer space friendly life extreme temperatures...</td>\n",
       "      <td>1.918411e-01</td>\n",
       "      <td>1.278941e-29</td>\n",
       "      <td>2.962610e-31</td>\n",
       "      <td>1.329413e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tennis original name lawn tennis game two oppo...</td>\n",
       "      <td>1.586850e-51</td>\n",
       "      <td>1.269482e-01</td>\n",
       "      <td>4.181668e-52</td>\n",
       "      <td>6.770790e-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one woman frequently flew southwest constantly...</td>\n",
       "      <td>5.944074e-37</td>\n",
       "      <td>1.188815e-35</td>\n",
       "      <td>4.261746e-02</td>\n",
       "      <td>1.545600e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>december almost seven years mers outbreak nove...</td>\n",
       "      <td>1.770790e-27</td>\n",
       "      <td>3.541581e-29</td>\n",
       "      <td>1.727140e-29</td>\n",
       "      <td>3.506044e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>life forms somehow find void soon die unless b...</td>\n",
       "      <td>1.897984e-01</td>\n",
       "      <td>7.591935e-21</td>\n",
       "      <td>2.272786e-19</td>\n",
       "      <td>6.183231e-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Modified Text       science  \\\n",
       "0  outer space friendly life extreme temperatures...  1.918411e-01   \n",
       "1  tennis original name lawn tennis game two oppo...  1.586850e-51   \n",
       "2  one woman frequently flew southwest constantly...  5.944074e-37   \n",
       "3  december almost seven years mers outbreak nove...  1.770790e-27   \n",
       "4  life forms somehow find void soon die unless b...  1.897984e-01   \n",
       "\n",
       "         sports      business         covid  \n",
       "0  1.278941e-29  2.962610e-31  1.329413e-27  \n",
       "1  1.269482e-01  4.181668e-52  6.770790e-51  \n",
       "2  1.188815e-35  4.261746e-02  1.545600e-38  \n",
       "3  3.541581e-29  1.727140e-29  3.506044e-01  \n",
       "4  7.591935e-21  2.272786e-19  6.183231e-19  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior_df.head(5) #Posterior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Sentence Completion\n",
    "\n",
    "Consider the datasets \"40.csv\" and \"10.csv\". In each sentence of \"10.csv\", the last word is not provided. The task is to predict it based on the remaining words in the sentence. Build your vocabulary from \"40.csv\" (except stop words). Assume that the missing words are part of this vocabulary. Consider this as a classification problem, where each word in the vocabulary may be considered as a class label. Use the Naive Bayes classifier to make probabilistic estimates of the missing words.\n",
    "\n",
    "i) Create the vocabulary without stop-words [2 marks]\n",
    "\n",
    "ii) Estimate the prior probabilities of all \"labels\", i.e. words in vocabulary [3 marks]\n",
    "\n",
    "iii) Estimate the class-conditional probabilities of all words [3 marks]\n",
    "\n",
    "iv) In each test sentence, calculate the most likely word in the missing position along with probability [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('40.csv')\n",
    "test_df = pd.read_csv('10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Too much protein can be harmful to people with kidney disease, but the latest research suggests that many of us need more high-quality protein, especially as we age.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text[7] #analyzing the sentence format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dehydration causes tiredness, low energy and head aches. Drink plenty of ______'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.text[5] #analyzing the test format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the vocabulary without stop-words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"modified_text\"] = train_df[\"text\"].apply(preprocess) #By using the function made in previous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_words = []\n",
    "for i in train_df.modified_text:\n",
    "    words = i.split()\n",
    "    x = len(words)\n",
    "    last_words.append(words[x-1])\n",
    "    \n",
    "last_words_dic = Counter(last_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lst = []\n",
    "\n",
    "for j in train_df.modified_text:\n",
    "    vocab_lst = vocab_lst + j.split()\n",
    "    \n",
    "vocab_dict = Counter(vocab_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['midst',\n",
       " 'covid',\n",
       " 'pandemic',\n",
       " 'eating',\n",
       " 'healthy',\n",
       " 'food',\n",
       " 'remains',\n",
       " 'important',\n",
       " 'part',\n",
       " 'maintaining',\n",
       " 'health',\n",
       " 'specific',\n",
       " 'foods',\n",
       " 'help',\n",
       " 'protect',\n",
       " 'virus',\n",
       " 'nutritious',\n",
       " 'diet',\n",
       " 'boost',\n",
       " 'immune',\n",
       " 'system',\n",
       " 'help',\n",
       " 'fight',\n",
       " 'symptoms',\n",
       " 'may',\n",
       " 'able',\n",
       " 'share',\n",
       " 'meals',\n",
       " 'friends',\n",
       " 'loved',\n",
       " 'ones',\n",
       " 'lots',\n",
       " 'ways',\n",
       " 'eat',\n",
       " 'well',\n",
       " 'support',\n",
       " 'health',\n",
       " 'difficult',\n",
       " 'time',\n",
       " 'eating',\n",
       " 'healthy',\n",
       " 'diet',\n",
       " 'strict',\n",
       " 'limitations',\n",
       " 'staying',\n",
       " 'unrealistically',\n",
       " 'thin',\n",
       " 'depriving',\n",
       " 'foods',\n",
       " 'love',\n",
       " 'rather',\n",
       " 'feeling',\n",
       " 'great',\n",
       " 'energy',\n",
       " 'improving',\n",
       " 'health',\n",
       " 'boosting',\n",
       " 'mood',\n",
       " 'healthy',\n",
       " 'eating',\n",
       " 'overly',\n",
       " 'complicated',\n",
       " 'feel',\n",
       " 'overwhelmed',\n",
       " 'conflicting',\n",
       " 'nutrition',\n",
       " 'diet',\n",
       " 'advice',\n",
       " 'alone',\n",
       " 'need',\n",
       " 'balance',\n",
       " 'protein',\n",
       " 'fat',\n",
       " 'carbohydrates',\n",
       " 'fiber',\n",
       " 'vitamins',\n",
       " 'minerals',\n",
       " 'diets',\n",
       " 'sustain',\n",
       " 'healthy',\n",
       " 'body',\n",
       " 'protein',\n",
       " 'gives',\n",
       " 'energy',\n",
       " 'get',\n",
       " 'go',\n",
       " 'keep',\n",
       " 'going',\n",
       " 'also',\n",
       " 'supporting',\n",
       " 'mood',\n",
       " 'cognitive',\n",
       " 'function',\n",
       " 'much',\n",
       " 'protein',\n",
       " 'harmful',\n",
       " 'people',\n",
       " 'kidney',\n",
       " 'disease',\n",
       " 'latest',\n",
       " 'research',\n",
       " 'suggests',\n",
       " 'many',\n",
       " 'us',\n",
       " 'need',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'protein',\n",
       " 'especially',\n",
       " 'age',\n",
       " 'bad',\n",
       " 'fats',\n",
       " 'wreck',\n",
       " 'diet',\n",
       " 'increase',\n",
       " 'risk',\n",
       " 'certain',\n",
       " 'diseases',\n",
       " 'good',\n",
       " 'fats',\n",
       " 'protect',\n",
       " 'brain',\n",
       " 'heart',\n",
       " 'fact',\n",
       " 'healthy',\n",
       " 'fats',\n",
       " 'omega',\n",
       " 'vital',\n",
       " 'physical',\n",
       " 'emotional',\n",
       " 'health',\n",
       " 'including',\n",
       " 'healthy',\n",
       " 'fat',\n",
       " 'diet',\n",
       " 'help',\n",
       " 'improve',\n",
       " 'mood',\n",
       " 'boost',\n",
       " 'well',\n",
       " 'even',\n",
       " 'trim',\n",
       " 'waistline',\n",
       " 'eating',\n",
       " 'foods',\n",
       " 'high',\n",
       " 'dietary',\n",
       " 'fiber',\n",
       " 'grains',\n",
       " 'fruit',\n",
       " 'vegetables',\n",
       " 'nuts',\n",
       " 'beans',\n",
       " 'help',\n",
       " 'stay',\n",
       " 'regular',\n",
       " 'lower',\n",
       " 'risk',\n",
       " 'heart',\n",
       " 'disease',\n",
       " 'stroke',\n",
       " 'diabetes',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'skin',\n",
       " 'even',\n",
       " 'help',\n",
       " 'lose',\n",
       " 'weight',\n",
       " 'well',\n",
       " 'leading',\n",
       " 'osteoporosis',\n",
       " 'getting',\n",
       " 'enough',\n",
       " 'calcium',\n",
       " 'diet',\n",
       " 'also',\n",
       " 'contribute',\n",
       " 'anxiety',\n",
       " 'depression',\n",
       " 'sleep',\n",
       " 'difficulties',\n",
       " 'whatever',\n",
       " 'age',\n",
       " 'gender',\n",
       " 'vital',\n",
       " 'include',\n",
       " 'calcium',\n",
       " 'rich',\n",
       " 'foods',\n",
       " 'diet',\n",
       " 'limit',\n",
       " 'deplete',\n",
       " 'calcium',\n",
       " 'get',\n",
       " 'enough',\n",
       " 'magnesium',\n",
       " 'vitamins',\n",
       " 'k',\n",
       " 'help',\n",
       " 'calcium',\n",
       " 'job',\n",
       " 'carbohydrates',\n",
       " 'one',\n",
       " 'body',\n",
       " 'main',\n",
       " 'sources',\n",
       " 'energy',\n",
       " 'come',\n",
       " 'complex',\n",
       " 'unrefined',\n",
       " 'carbs',\n",
       " 'vegetables',\n",
       " 'whole',\n",
       " 'grains',\n",
       " 'fruit',\n",
       " 'rather',\n",
       " 'sugars',\n",
       " 'refined',\n",
       " 'carbs',\n",
       " 'cutting',\n",
       " 'back',\n",
       " 'white',\n",
       " 'bread',\n",
       " 'pastries',\n",
       " 'starches',\n",
       " 'sugar',\n",
       " 'prevent',\n",
       " 'rapid',\n",
       " 'spikes',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'fluctuations',\n",
       " 'mood',\n",
       " 'energy',\n",
       " 'build',\n",
       " 'fat',\n",
       " 'especially',\n",
       " 'around',\n",
       " 'waistline',\n",
       " 'switching',\n",
       " 'healthy',\n",
       " 'diet',\n",
       " 'nothing',\n",
       " 'proposition',\n",
       " 'perfect',\n",
       " 'completely',\n",
       " 'eliminate',\n",
       " 'foods',\n",
       " 'enjoy',\n",
       " 'change',\n",
       " 'everything',\n",
       " 'usually',\n",
       " 'leads',\n",
       " 'cheating',\n",
       " 'giving',\n",
       " 'new',\n",
       " 'eating',\n",
       " 'plan',\n",
       " 'think',\n",
       " 'planning',\n",
       " 'healthy',\n",
       " 'diet',\n",
       " 'number',\n",
       " 'small',\n",
       " 'manageable',\n",
       " 'steps',\n",
       " 'like',\n",
       " 'adding',\n",
       " 'salad',\n",
       " 'diet',\n",
       " 'day',\n",
       " 'small',\n",
       " 'changes',\n",
       " 'become',\n",
       " 'habit',\n",
       " 'continue',\n",
       " 'add',\n",
       " 'healthy',\n",
       " 'choices',\n",
       " 'cooking',\n",
       " 'meals',\n",
       " 'home',\n",
       " 'help',\n",
       " 'take',\n",
       " 'charge',\n",
       " 'eating',\n",
       " 'better',\n",
       " 'monitor',\n",
       " 'exactly',\n",
       " 'goes',\n",
       " 'food',\n",
       " 'eat',\n",
       " 'fewer',\n",
       " 'calories',\n",
       " 'avoid',\n",
       " 'chemical',\n",
       " 'additives',\n",
       " 'added',\n",
       " 'sugar',\n",
       " 'unhealthy',\n",
       " 'fats',\n",
       " 'packaged',\n",
       " 'takeout',\n",
       " 'foods',\n",
       " 'leave',\n",
       " 'feeling',\n",
       " 'tired',\n",
       " 'bloated',\n",
       " 'irritable',\n",
       " 'exacerbate',\n",
       " 'symptoms',\n",
       " 'depression',\n",
       " 'stress',\n",
       " 'anxiety',\n",
       " 'cutting',\n",
       " 'back',\n",
       " 'unhealthy',\n",
       " 'foods',\n",
       " 'diet',\n",
       " 'important',\n",
       " 'replace',\n",
       " 'healthy',\n",
       " 'alternatives',\n",
       " 'replacing',\n",
       " 'dangerous',\n",
       " 'trans',\n",
       " 'fats',\n",
       " 'healthy',\n",
       " 'fats',\n",
       " 'switching',\n",
       " 'fried',\n",
       " 'chicken',\n",
       " 'grilled',\n",
       " 'salmon',\n",
       " 'make',\n",
       " 'positive',\n",
       " 'difference',\n",
       " 'health',\n",
       " 'important',\n",
       " 'aware',\n",
       " 'food',\n",
       " 'manufacturers',\n",
       " 'often',\n",
       " 'hide',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'sugar',\n",
       " 'unhealthy',\n",
       " 'fats',\n",
       " 'packaged',\n",
       " 'food',\n",
       " 'even',\n",
       " 'food',\n",
       " 'claiming',\n",
       " 'healthy',\n",
       " 'healthier',\n",
       " 'food',\n",
       " 'eat',\n",
       " 'better',\n",
       " 'feel',\n",
       " 'meal',\n",
       " 'junk',\n",
       " 'food',\n",
       " 'eat',\n",
       " 'likely',\n",
       " 'feel',\n",
       " 'uncomfortable',\n",
       " 'nauseous',\n",
       " 'drained',\n",
       " 'energy',\n",
       " 'drink',\n",
       " 'plenty',\n",
       " 'water',\n",
       " 'water',\n",
       " 'helps',\n",
       " 'flush',\n",
       " 'systems',\n",
       " 'waste',\n",
       " 'products',\n",
       " 'toxins',\n",
       " 'yet',\n",
       " 'many',\n",
       " 'us',\n",
       " 'go',\n",
       " 'life',\n",
       " 'dehydrated',\n",
       " 'causing',\n",
       " 'tiredness',\n",
       " 'low',\n",
       " 'energy',\n",
       " 'headaches',\n",
       " 'common',\n",
       " 'mistake',\n",
       " 'thirst',\n",
       " 'hunger',\n",
       " 'staying',\n",
       " 'well',\n",
       " 'hydrated',\n",
       " 'also',\n",
       " 'help',\n",
       " 'make',\n",
       " 'healthier',\n",
       " 'food',\n",
       " 'choices',\n",
       " 'moderation',\n",
       " 'essence',\n",
       " 'means',\n",
       " 'eating',\n",
       " 'much',\n",
       " 'food',\n",
       " 'body',\n",
       " 'needs',\n",
       " 'feel',\n",
       " 'satisfied',\n",
       " 'end',\n",
       " 'meal',\n",
       " 'stuffed',\n",
       " 'many',\n",
       " 'us',\n",
       " 'moderation',\n",
       " 'means',\n",
       " 'eating',\n",
       " 'less',\n",
       " 'mean',\n",
       " 'eliminating',\n",
       " 'foods',\n",
       " 'love',\n",
       " 'eating',\n",
       " 'bacon',\n",
       " 'breakfast',\n",
       " 'week',\n",
       " 'example',\n",
       " 'could',\n",
       " 'considered',\n",
       " 'moderation',\n",
       " 'follow',\n",
       " 'healthy',\n",
       " 'lunch',\n",
       " 'dinner',\n",
       " 'follow',\n",
       " 'box',\n",
       " 'donuts',\n",
       " 'sausage',\n",
       " 'pizza',\n",
       " 'eating',\n",
       " 'alone',\n",
       " 'especially',\n",
       " 'front',\n",
       " 'tv',\n",
       " 'computer',\n",
       " 'often',\n",
       " 'leads',\n",
       " 'mindless',\n",
       " 'overeating',\n",
       " 'control',\n",
       " 'emotional',\n",
       " 'eating',\n",
       " 'always',\n",
       " 'eat',\n",
       " 'satisfy',\n",
       " 'hunger',\n",
       " 'many',\n",
       " 'us',\n",
       " 'also',\n",
       " 'turn',\n",
       " 'food',\n",
       " 'relieve',\n",
       " 'stress',\n",
       " 'cope',\n",
       " 'unpleasant',\n",
       " 'emotions',\n",
       " 'sadness',\n",
       " 'loneliness',\n",
       " 'boredom',\n",
       " 'fruit',\n",
       " 'vegetables',\n",
       " 'low',\n",
       " 'calories',\n",
       " 'nutrient',\n",
       " 'dense',\n",
       " 'means',\n",
       " 'packed',\n",
       " 'vitamins',\n",
       " 'minerals',\n",
       " 'antioxidants',\n",
       " 'fiber',\n",
       " 'focus',\n",
       " 'eating',\n",
       " 'recommended',\n",
       " 'daily',\n",
       " 'amount',\n",
       " 'least',\n",
       " 'five',\n",
       " 'servings',\n",
       " 'fruit',\n",
       " 'vegetables',\n",
       " 'naturally',\n",
       " 'fill',\n",
       " 'help',\n",
       " 'cut',\n",
       " 'back',\n",
       " 'unhealthy',\n",
       " 'foods',\n",
       " 'satisfy',\n",
       " 'sweet',\n",
       " 'tooth',\n",
       " 'naturally',\n",
       " 'sweet',\n",
       " 'vegetables',\n",
       " 'carrots',\n",
       " 'beets',\n",
       " 'sweet',\n",
       " 'potatoes',\n",
       " 'yams',\n",
       " 'onions',\n",
       " 'bell',\n",
       " 'peppers',\n",
       " 'squash',\n",
       " 'add',\n",
       " 'sweetness',\n",
       " 'meals',\n",
       " 'reduce',\n",
       " 'cravings',\n",
       " 'added',\n",
       " 'sugar',\n",
       " 'cook',\n",
       " 'green',\n",
       " 'beans',\n",
       " 'broccoli',\n",
       " 'brussels',\n",
       " 'sprouts',\n",
       " 'asparagus',\n",
       " 'new',\n",
       " 'ways',\n",
       " 'instead',\n",
       " 'boiling',\n",
       " 'steaming',\n",
       " 'healthy',\n",
       " 'sides',\n",
       " 'try',\n",
       " 'grilling',\n",
       " 'roasting',\n",
       " 'pan',\n",
       " 'frying',\n",
       " 'chili',\n",
       " 'flakes',\n",
       " 'garlic',\n",
       " 'shallots',\n",
       " 'mushrooms',\n",
       " 'onion',\n",
       " 'plain',\n",
       " 'salads',\n",
       " 'steamed',\n",
       " 'veggies',\n",
       " 'quickly',\n",
       " 'become',\n",
       " 'bland',\n",
       " 'plenty',\n",
       " 'ways',\n",
       " 'add',\n",
       " 'taste',\n",
       " 'vegetable',\n",
       " 'dishes']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lst #Vocabulary List without stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimate the prior probabilities of all \"labels\", i.e. words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dict = {}\n",
    "total_words = sum(vocab_dict.values())\n",
    "\n",
    "for word in vocab_dict.keys():\n",
    "     prior_dict[word] = [vocab_dict[word], vocab_dict[word]/total_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\"Occurance\", \"Frequency\"]\n",
    "\n",
    "prior_df = pd.DataFrame(prior_dict, index = index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Prior Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midst</th>\n",
       "      <th>covid</th>\n",
       "      <th>pandemic</th>\n",
       "      <th>eating</th>\n",
       "      <th>healthy</th>\n",
       "      <th>food</th>\n",
       "      <th>remains</th>\n",
       "      <th>important</th>\n",
       "      <th>part</th>\n",
       "      <th>maintaining</th>\n",
       "      <th>...</th>\n",
       "      <th>onion</th>\n",
       "      <th>plain</th>\n",
       "      <th>salads</th>\n",
       "      <th>steamed</th>\n",
       "      <th>veggies</th>\n",
       "      <th>quickly</th>\n",
       "      <th>bland</th>\n",
       "      <th>taste</th>\n",
       "      <th>vegetable</th>\n",
       "      <th>dishes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Occurance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.01773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              midst     covid  pandemic     eating    healthy      food  \\\n",
       "Occurance  1.000000  1.000000  1.000000  12.000000  14.000000  10.00000   \n",
       "Frequency  0.001773  0.001773  0.001773   0.021277   0.024823   0.01773   \n",
       "\n",
       "            remains  important      part  maintaining  ...     onion  \\\n",
       "Occurance  1.000000   3.000000  1.000000     1.000000  ...  1.000000   \n",
       "Frequency  0.001773   0.005319  0.001773     0.001773  ...  0.001773   \n",
       "\n",
       "              plain    salads   steamed   veggies   quickly     bland  \\\n",
       "Occurance  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "Frequency  0.001773  0.001773  0.001773  0.001773  0.001773  0.001773   \n",
       "\n",
       "              taste  vegetable    dishes  \n",
       "Occurance  1.000000   1.000000  1.000000  \n",
       "Frequency  0.001773   0.001773  0.001773  \n",
       "\n",
       "[2 rows x 367 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimate the class-conditional probabilities of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_distribution_words(word, modified_text):\n",
    "    conditional_lst = []\n",
    "\n",
    "    for j in vocab_dict.keys():\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        for i in modified_text:\n",
    "            words = i.split()\n",
    "            \n",
    "            if j in words and word in words:\n",
    "                count = count + 1\n",
    "                \n",
    "                    \n",
    "        if count/vocab_dict[j] == 0:\n",
    "            conditional_lst.append(0.01)  #to avoid error in distrbution, replacing zero with 0.01\n",
    "        else:\n",
    "            conditional_lst.append(count/vocab_dict[j])\n",
    "            \n",
    "    return conditional_lst    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_dict = {}\n",
    "for word in vocab_dict.keys():\n",
    "    conditional_dict[word] = conditional_distribution_words(word, train_df.modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_df = pd.DataFrame(conditional_dict, index = list(vocab_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Class-conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midst</th>\n",
       "      <th>covid</th>\n",
       "      <th>pandemic</th>\n",
       "      <th>eating</th>\n",
       "      <th>healthy</th>\n",
       "      <th>food</th>\n",
       "      <th>remains</th>\n",
       "      <th>important</th>\n",
       "      <th>part</th>\n",
       "      <th>maintaining</th>\n",
       "      <th>...</th>\n",
       "      <th>onion</th>\n",
       "      <th>plain</th>\n",
       "      <th>salads</th>\n",
       "      <th>steamed</th>\n",
       "      <th>veggies</th>\n",
       "      <th>quickly</th>\n",
       "      <th>bland</th>\n",
       "      <th>taste</th>\n",
       "      <th>vegetable</th>\n",
       "      <th>dishes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>midst</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandemic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eating</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthy</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quickly</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bland</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taste</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vegetable</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dishes</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              midst     covid  pandemic    eating   healthy      food  \\\n",
       "midst      1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "covid      1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "pandemic   1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "eating     0.083333  0.083333  0.083333  1.000000  0.333333  0.333333   \n",
       "healthy    0.071429  0.071429  0.071429  0.285714  1.000000  0.142857   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "quickly    0.010000  0.010000  0.010000  0.010000  0.010000  0.010000   \n",
       "bland      0.010000  0.010000  0.010000  0.010000  0.010000  0.010000   \n",
       "taste      0.010000  0.010000  0.010000  0.010000  0.010000  0.010000   \n",
       "vegetable  0.010000  0.010000  0.010000  0.010000  0.010000  0.010000   \n",
       "dishes     0.010000  0.010000  0.010000  0.010000  0.010000  0.010000   \n",
       "\n",
       "            remains  important      part  maintaining  ...     onion  plain  \\\n",
       "midst      1.000000   1.000000  1.000000     1.000000  ...  0.010000   0.01   \n",
       "covid      1.000000   1.000000  1.000000     1.000000  ...  0.010000   0.01   \n",
       "pandemic   1.000000   1.000000  1.000000     1.000000  ...  0.010000   0.01   \n",
       "eating     0.083333   0.083333  0.083333     0.083333  ...  0.010000   0.01   \n",
       "healthy    0.071429   0.214286  0.071429     0.071429  ...  0.071429   0.01   \n",
       "...             ...        ...       ...          ...  ...       ...    ...   \n",
       "quickly    0.010000   0.010000  0.010000     0.010000  ...  0.010000   1.00   \n",
       "bland      0.010000   0.010000  0.010000     0.010000  ...  0.010000   1.00   \n",
       "taste      0.010000   0.010000  0.010000     0.010000  ...  0.010000   1.00   \n",
       "vegetable  0.010000   0.010000  0.010000     0.010000  ...  0.010000   1.00   \n",
       "dishes     0.010000   0.010000  0.010000     0.010000  ...  0.010000   1.00   \n",
       "\n",
       "           salads  steamed  veggies  quickly  bland  taste  vegetable  dishes  \n",
       "midst        0.01     0.01     0.01     0.01   0.01   0.01       0.01    0.01  \n",
       "covid        0.01     0.01     0.01     0.01   0.01   0.01       0.01    0.01  \n",
       "pandemic     0.01     0.01     0.01     0.01   0.01   0.01       0.01    0.01  \n",
       "eating       0.01     0.01     0.01     0.01   0.01   0.01       0.01    0.01  \n",
       "healthy      0.01     0.01     0.01     0.01   0.01   0.01       0.01    0.01  \n",
       "...           ...      ...      ...      ...    ...    ...        ...     ...  \n",
       "quickly      1.00     1.00     1.00     1.00   1.00   1.00       1.00    1.00  \n",
       "bland        1.00     1.00     1.00     1.00   1.00   1.00       1.00    1.00  \n",
       "taste        1.00     1.00     1.00     1.00   1.00   1.00       1.00    1.00  \n",
       "vegetable    1.00     1.00     1.00     1.00   1.00   1.00       1.00    1.00  \n",
       "dishes       1.00     1.00     1.00     1.00   1.00   1.00       1.00    1.00  \n",
       "\n",
       "[367 rows x 367 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. In each test sentence, calculate the most likely word in the missing position along with probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_distribution_words(data):\n",
    "    \n",
    "    posterior_val = []\n",
    "    \n",
    "    for j in range(len(vocab_dict.keys())):\n",
    "        \n",
    "        res = 1\n",
    "        \n",
    "        for word in data.split():\n",
    "            if word in vocab_dict.keys():\n",
    "\n",
    "                row_data = np.array(conditional_df.loc[word][0:len(vocab_dict.keys())])\n",
    "\n",
    "                res = res * (prior_df[word][\"Frequency\"]*row_data[j]/np.dot(row_data, np.array(list(prior_df.loc[\"Frequency\"]))))\n",
    "                \n",
    "        posterior_val.append((res))\n",
    "        \n",
    "    return posterior_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Apllied Posterior Distribution on the given test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_text = posterior_distribution_words(\"dehydration causes tiredness low energy head aches drink plenty\") #Just trying with help of single text\n",
    "\n",
    "test_df[\"modified_text\"] = train_df[\"text\"].apply(preprocess) #By using the function made in previous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>modified_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eating healthy food is important for maintainn...</td>\n",
       "      <td>midst covid pandemic eating healthy food remai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Following a healthy diet will boost your _____</td>\n",
       "      <td>specific foods help protect virus nutritious d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avoid eating chemical additives, added sugars ...</td>\n",
       "      <td>may able share meals friends loved ones lots w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Eating healthy food is important for maintainn...   \n",
       "1     Following a healthy diet will boost your _____   \n",
       "2  Avoid eating chemical additives, added sugars ...   \n",
       "\n",
       "                                       modified_text  \n",
       "0  midst covid pandemic eating healthy food remai...  \n",
       "1  specific foods help protect virus nutritious d...  \n",
       "2  may able share meals friends loved ones lots w...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_lst = []\n",
    "for text in test_df.modified_text:\n",
    "    posterior_lst.append(posterior_distribution_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_df = pd.DataFrame(posterior_lst, columns = list(vocab_dict.keys()))\n",
    "\n",
    "posterior_df.insert(0, \"Modified Text\", test_df.modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Posterior Distribution (On text of test cases for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modified Text</th>\n",
       "      <th>midst</th>\n",
       "      <th>covid</th>\n",
       "      <th>pandemic</th>\n",
       "      <th>eating</th>\n",
       "      <th>healthy</th>\n",
       "      <th>food</th>\n",
       "      <th>remains</th>\n",
       "      <th>important</th>\n",
       "      <th>part</th>\n",
       "      <th>...</th>\n",
       "      <th>onion</th>\n",
       "      <th>plain</th>\n",
       "      <th>salads</th>\n",
       "      <th>steamed</th>\n",
       "      <th>veggies</th>\n",
       "      <th>quickly</th>\n",
       "      <th>bland</th>\n",
       "      <th>taste</th>\n",
       "      <th>vegetable</th>\n",
       "      <th>dishes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>midst covid pandemic eating healthy food remai...</td>\n",
       "      <td>1.376501e-19</td>\n",
       "      <td>1.376501e-19</td>\n",
       "      <td>1.376501e-19</td>\n",
       "      <td>5.285763e-17</td>\n",
       "      <td>1.850017e-16</td>\n",
       "      <td>1.761921e-17</td>\n",
       "      <td>1.376501e-19</td>\n",
       "      <td>2.477701e-18</td>\n",
       "      <td>1.376501e-19</td>\n",
       "      <td>...</td>\n",
       "      <td>2.477701e-36</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "      <td>3.468782e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>specific foods help protect virus nutritious d...</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.269052e-37</td>\n",
       "      <td>8.460347e-37</td>\n",
       "      <td>5.583829e-41</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.142147e-41</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "      <td>1.130725e-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>may able share meals friends loved ones lots w...</td>\n",
       "      <td>4.106363e-50</td>\n",
       "      <td>4.106363e-50</td>\n",
       "      <td>4.106363e-50</td>\n",
       "      <td>5.475150e-47</td>\n",
       "      <td>1.368788e-46</td>\n",
       "      <td>2.053181e-45</td>\n",
       "      <td>4.106363e-50</td>\n",
       "      <td>4.106363e-50</td>\n",
       "      <td>4.106363e-50</td>\n",
       "      <td>...</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "      <td>6.843938e-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eating healthy diet strict limitations staying...</td>\n",
       "      <td>1.439837e-62</td>\n",
       "      <td>1.439837e-62</td>\n",
       "      <td>1.439837e-62</td>\n",
       "      <td>7.271906e-32</td>\n",
       "      <td>2.036134e-31</td>\n",
       "      <td>9.598916e-59</td>\n",
       "      <td>1.439837e-62</td>\n",
       "      <td>4.363144e-60</td>\n",
       "      <td>1.439837e-62</td>\n",
       "      <td>...</td>\n",
       "      <td>8.639025e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "      <td>1.209463e-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>healthy eating overly complicated feel overwhe...</td>\n",
       "      <td>2.250841e-35</td>\n",
       "      <td>2.250841e-35</td>\n",
       "      <td>2.250841e-35</td>\n",
       "      <td>4.910925e-17</td>\n",
       "      <td>8.594119e-17</td>\n",
       "      <td>9.003363e-33</td>\n",
       "      <td>2.250841e-35</td>\n",
       "      <td>6.138657e-34</td>\n",
       "      <td>2.250841e-35</td>\n",
       "      <td>...</td>\n",
       "      <td>2.701009e-36</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "      <td>3.781412e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>need balance protein fat carbohydrates fiber v...</td>\n",
       "      <td>7.020668e-39</td>\n",
       "      <td>7.020668e-39</td>\n",
       "      <td>7.020668e-39</td>\n",
       "      <td>3.120297e-35</td>\n",
       "      <td>7.584055e-19</td>\n",
       "      <td>4.680445e-37</td>\n",
       "      <td>7.020668e-39</td>\n",
       "      <td>2.106200e-38</td>\n",
       "      <td>7.020668e-39</td>\n",
       "      <td>...</td>\n",
       "      <td>7.020668e-39</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "      <td>9.828935e-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>protein gives energy get go keep going also su...</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>3.086605e-36</td>\n",
       "      <td>3.858256e-36</td>\n",
       "      <td>1.234642e-37</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "      <td>1.851963e-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>much protein harmful people kidney disease lat...</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.521589e-47</td>\n",
       "      <td>4.564766e-53</td>\n",
       "      <td>4.564766e-53</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>...</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "      <td>1.460725e-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bad fats wreck diet increase risk certain dise...</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>3.192529e-39</td>\n",
       "      <td>7.036594e-39</td>\n",
       "      <td>1.433380e-41</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>1.303073e-40</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>...</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "      <td>7.023563e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fact healthy fats omega vital physical emotion...</td>\n",
       "      <td>2.508973e-24</td>\n",
       "      <td>2.508973e-24</td>\n",
       "      <td>2.508973e-24</td>\n",
       "      <td>1.003589e-21</td>\n",
       "      <td>1.505384e-11</td>\n",
       "      <td>3.584247e-21</td>\n",
       "      <td>2.508973e-24</td>\n",
       "      <td>1.075274e-22</td>\n",
       "      <td>2.508973e-24</td>\n",
       "      <td>...</td>\n",
       "      <td>1.254486e-25</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "      <td>1.756281e-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Modified Text         midst  \\\n",
       "0  midst covid pandemic eating healthy food remai...  1.376501e-19   \n",
       "1  specific foods help protect virus nutritious d...  1.130725e-43   \n",
       "2  may able share meals friends loved ones lots w...  4.106363e-50   \n",
       "3  eating healthy diet strict limitations staying...  1.439837e-62   \n",
       "4  healthy eating overly complicated feel overwhe...  2.250841e-35   \n",
       "5  need balance protein fat carbohydrates fiber v...  7.020668e-39   \n",
       "6  protein gives energy get go keep going also su...  1.851963e-40   \n",
       "7  much protein harmful people kidney disease lat...  1.460725e-57   \n",
       "8  bad fats wreck diet increase risk certain dise...  7.023563e-44   \n",
       "9  fact healthy fats omega vital physical emotion...  2.508973e-24   \n",
       "\n",
       "          covid      pandemic        eating       healthy          food  \\\n",
       "0  1.376501e-19  1.376501e-19  5.285763e-17  1.850017e-16  1.761921e-17   \n",
       "1  1.130725e-43  1.130725e-43  1.269052e-37  8.460347e-37  5.583829e-41   \n",
       "2  4.106363e-50  4.106363e-50  5.475150e-47  1.368788e-46  2.053181e-45   \n",
       "3  1.439837e-62  1.439837e-62  7.271906e-32  2.036134e-31  9.598916e-59   \n",
       "4  2.250841e-35  2.250841e-35  4.910925e-17  8.594119e-17  9.003363e-33   \n",
       "5  7.020668e-39  7.020668e-39  3.120297e-35  7.584055e-19  4.680445e-37   \n",
       "6  1.851963e-40  1.851963e-40  3.086605e-36  3.858256e-36  1.234642e-37   \n",
       "7  1.460725e-57  1.460725e-57  1.521589e-47  4.564766e-53  4.564766e-53   \n",
       "8  7.023563e-44  7.023563e-44  3.192529e-39  7.036594e-39  1.433380e-41   \n",
       "9  2.508973e-24  2.508973e-24  1.003589e-21  1.505384e-11  3.584247e-21   \n",
       "\n",
       "        remains     important          part  ...         onion         plain  \\\n",
       "0  1.376501e-19  2.477701e-18  1.376501e-19  ...  2.477701e-36  3.468782e-37   \n",
       "1  1.130725e-43  1.142147e-41  1.130725e-43  ...  1.130725e-43  1.130725e-43   \n",
       "2  4.106363e-50  4.106363e-50  4.106363e-50  ...  6.843938e-50  6.843938e-50   \n",
       "3  1.439837e-62  4.363144e-60  1.439837e-62  ...  8.639025e-65  1.209463e-65   \n",
       "4  2.250841e-35  6.138657e-34  2.250841e-35  ...  2.701009e-36  3.781412e-37   \n",
       "5  7.020668e-39  2.106200e-38  7.020668e-39  ...  7.020668e-39  9.828935e-40   \n",
       "6  1.851963e-40  1.851963e-40  1.851963e-40  ...  1.851963e-40  1.851963e-40   \n",
       "7  1.460725e-57  1.460725e-57  1.460725e-57  ...  1.460725e-57  1.460725e-57   \n",
       "8  7.023563e-44  1.303073e-40  7.023563e-44  ...  7.023563e-44  7.023563e-44   \n",
       "9  2.508973e-24  1.075274e-22  2.508973e-24  ...  1.254486e-25  1.756281e-26   \n",
       "\n",
       "         salads       steamed       veggies       quickly         bland  \\\n",
       "0  3.468782e-37  3.468782e-37  3.468782e-37  3.468782e-37  3.468782e-37   \n",
       "1  1.130725e-43  1.130725e-43  1.130725e-43  1.130725e-43  1.130725e-43   \n",
       "2  6.843938e-50  6.843938e-50  6.843938e-50  6.843938e-50  6.843938e-50   \n",
       "3  1.209463e-65  1.209463e-65  1.209463e-65  1.209463e-65  1.209463e-65   \n",
       "4  3.781412e-37  3.781412e-37  3.781412e-37  3.781412e-37  3.781412e-37   \n",
       "5  9.828935e-40  9.828935e-40  9.828935e-40  9.828935e-40  9.828935e-40   \n",
       "6  1.851963e-40  1.851963e-40  1.851963e-40  1.851963e-40  1.851963e-40   \n",
       "7  1.460725e-57  1.460725e-57  1.460725e-57  1.460725e-57  1.460725e-57   \n",
       "8  7.023563e-44  7.023563e-44  7.023563e-44  7.023563e-44  7.023563e-44   \n",
       "9  1.756281e-26  1.756281e-26  1.756281e-26  1.756281e-26  1.756281e-26   \n",
       "\n",
       "          taste     vegetable        dishes  \n",
       "0  3.468782e-37  3.468782e-37  3.468782e-37  \n",
       "1  1.130725e-43  1.130725e-43  1.130725e-43  \n",
       "2  6.843938e-50  6.843938e-50  6.843938e-50  \n",
       "3  1.209463e-65  1.209463e-65  1.209463e-65  \n",
       "4  3.781412e-37  3.781412e-37  3.781412e-37  \n",
       "5  9.828935e-40  9.828935e-40  9.828935e-40  \n",
       "6  1.851963e-40  1.851963e-40  1.851963e-40  \n",
       "7  1.460725e-57  1.460725e-57  1.460725e-57  \n",
       "8  7.023563e-44  7.023563e-44  7.023563e-44  \n",
       "9  1.756281e-26  1.756281e-26  1.756281e-26  \n",
       "\n",
       "[10 rows x 368 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
